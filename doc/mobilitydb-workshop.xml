<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN"
"http://www.oasis-open.org/docbook/xml/4.5/docbookx.dtd" [

<!ENTITY geography_support
"<inlinemediaobject>
	<imageobject><imagedata fileref='images/world.pdf' width='16pt' /></imageobject>
	<imageobject><imagedata fileref='images/world.svg' width='16pt' /></imageobject>
	<imageobject><imagedata fileref='images/world.png' width='16pt' /></imageobject>
 </inlinemediaobject>
">

<!ENTITY Z_support
"<inlinemediaobject>
	<imageobject><imagedata fileref='images/cube.pdf' width='16pt' /></imageobject>
	<imageobject><imagedata fileref='images/cube.svg' width='16pt' /></imageobject>
	<imageobject><imagedata fileref='images/cube.png' width='16pt' /></imageobject>
 </inlinemediaobject>
">

<!ENTITY python_support
"<inlinemediaobject>
	<imageobject><imagedata fileref='images/python.pdf' width='16pt' /></imageobject>
	<imageobject><imagedata fileref='images/python.svg' width='16pt' /></imageobject>
	<imageobject><imagedata fileref='images/python.png' width='16pt' /></imageobject>
 </inlinemediaobject>
">
]>

<book lang="en">
	<bookinfo>
		<title>MobilityDB Data Input Workshop</title>

		<author>
			<firstname>Mahmoud</firstname>
			<surname>SAKR</surname>
			<affiliation>
				<orgname>Universit&eacute; Libre de Bruxelles, Belgium</orgname>
			</affiliation>
		</author>

		<author>
			<firstname>Esteban</firstname>
			<surname>ZIM&Aacute;NYI</surname>
			<affiliation>
				<orgname>Universit&eacute; Libre de Bruxelles, Belgium</orgname>
			</affiliation>
		</author>

		<abstract>
			<para>
				Every module in this workshop illustrates a data input scenario of MobilityDB. The data sets and the tools are described inside each of the modules. Eventually, more modules will be added to discover more MobilityDB features.
			</para>
			<para>
				While this workshop illustrates the usage of MobilityDB functions, it does not explain them in detail. If you need help concerning the functions of MobilityDB, please refer to the <ulink url="https://docs.mobilitydb.com/nightly/">documentation</ulink>.
			</para>
			<para>
				If you have questions, ideas, comments, etc., please contact me on <ulink url="mailto:mahmoud.sakr@ulb.ac.be">mahmoud.sakr@ulb.ac.be</ulink>.
			</para>
			<para>
				<inlinemediaobject>
					<imageobject>
						<imagedata fileref='images/mobilitydb-logo.png' width='200pt' />
					</imageobject>
				</inlinemediaobject>
			</para>
		</abstract>
	</bookinfo>

	<chapter id ="AIS">
		<title>Managing Ship Trajectories (AIS)</title>

			<para> AIS stands for Automatic Identification System. It is the location tracking system for sea vessels. This module illustrates how to load big AIS data sets into MobilityDB and do basic exploration.
			</para>
			<para>
			The idea of this module is inspired from the tutorial of <ulink url="https://github.com/anitagraser/movingpandas">MovingPandas</ulink> on ship data analysis by Anita Graser.
			</para>

			<section>
				<title>Contents</title>
				<para>This module covers the following topics:
					<itemizedlist>
						<listitem>
							<para>Loading large trajectory datasets into MobilityDB</para>
						</listitem>
						<listitem>
							<para>Create proper indexes to speed up trajectory construction</para>
						</listitem>
						<listitem>
							<para>Select trajectories by a spatial window</para>
						</listitem>
						<listitem>
							<para>Join trajectories tables by proximity</para>
						</listitem>
						<listitem>
							<para>Select certain parts inside individual trajectories</para>
						</listitem>
						<listitem>
							<para>Manage the temporal speed and azimuth features of ships</para>
						</listitem>
					</itemizedlist>
				</para>
			</section>

			<section>
				<title>Data</title>
				<para>
					The Danish Maritime Authority publishes about 3 TB of AIS routes in CSV format <ulink url="https://www.dma.dk/SikkerhedTilSoes/Sejladsinformation/AIS/Sider/default.aspx">here</ulink>. The columns in the CSV are listed in <xref linkend="tabdata"/>. This module uses the data of one day April 1<superscript>st</superscript> 2018. The CSV file size is 1.9 GB, and it contains about 10 M rows.
				</para>

				<table pgwide='1' width='100%' id='tabdata' frame='all'><title>AIS columns</title>
					<tgroup cols='2' align='left' colsep='1' rowsep='1'>
						<colspec colwidth="1*" colname='c1'/>
						<colspec colwidth="3*" colname='c2'/>
						<tbody>
							<row> <entry>Timestamp</entry> <entry>Timestamp from the AIS base station, format: 31/12/2015 23:59:59</entry> </row>
							<row> <entry>Type of mobile</entry> <entry>Describes what type of target this message is received from (class A AIS Vessel, Class B AIS vessel, etc)</entry> </row>
							<row> <entry>MMSI</entry> <entry>MMSI number of vessel</entry> </row>
							<row> <entry>Latitude</entry> <entry>Latitude of message report (e.g. 57,8794)</entry> </row>
							<row> <entry>Longitude</entry> <entry>Longitude of message report (e.g. 17,9125)</entry> </row>
							<row> <entry>Navigational status</entry> <entry>Navigational status from AIS message if available, e.g.: 'Engaged in fishing', 'Under way using engine', mv.</entry> </row>
							<row> <entry>ROT</entry> <entry>Rot of turn from AIS message if available</entry> </row>
							<row> <entry>SOG</entry> <entry>Speed over ground from AIS message if available</entry> </row>
							<row> <entry>COG</entry> <entry>Course over ground from AIS message if available</entry> </row>
							<row> <entry>Heading</entry> <entry>Heading from AIS message if available</entry> </row>
							<row> <entry>IMO</entry> <entry>IMO number of the vessel</entry> </row>
							<row> <entry>Callsign</entry> <entry>Callsign of the vessel</entry> </row>
							<row> <entry>Name</entry> <entry>Name of the vessel</entry> </row>
							<row> <entry>Ship type</entry> <entry>Describes the AIS ship type of this vessel</entry> </row>
							<row> <entry>Cargo type</entry> <entry>Type of cargo from the AIS message</entry> </row>
							<row> <entry>Width</entry> <entry>Width of the vessel</entry> </row>
							<row> <entry>Length</entry> <entry>Lenght of the vessel</entry> </row>
							<row> <entry>Type of position fixing device</entry> <entry>Type of positional fixing device from the AIS message</entry> </row>
							<row> <entry>Draught</entry> <entry>Draugth field from AIS message</entry> </row>
							<row> <entry>Destination</entry> <entry>Destination from AIS message</entry> </row>
							<row> <entry>ETA</entry> <entry>Estimated Time of Arrival, if available</entry> </row>
							<row> <entry>Data source type</entry> <entry>Data source type, e.g. AIS</entry> </row>
							<row> <entry>Size A</entry> <entry>Length from GPS to the bow</entry> </row>
							<row> <entry>Size B</entry> <entry>Length from GPS to the stern</entry> </row>
							<row> <entry>Size C</entry> <entry>Length from GPS to starboard side</entry> </row>
							<row> <entry>Size D</entry> <entry>Length from GPS to port side</entry> </row>
						</tbody>
					</tgroup>
				</table>
			</section>

			<section>
				<title>Tools</title>
				<para>
					The tools used in this module are as follows:
					<itemizedlist>
						<listitem>
							<para>MobilityDB, on top of PostgreSQL and PostGIS. Here I use the MobilityDB <ulink url="https://github.com/ULB-CoDE-WIT/MobilityDB"> docker image</ulink>. </para>
						</listitem>
						<listitem>
							<para>QGIS</para>
						</listitem>
					</itemizedlist>
				</para>
			</section>

			<section>
				<title>Preparing the Database</title>
				<para>
					Create a new database <varname>DanishAIS</varname>, then use your SQL editor to create the MobilityDB extension as follows:
					<programlisting>
CREATE EXTENSION MobilityDB CASCADE;
					</programlisting>
					The <varname>CASCADE</varname> command will additionally create the PostGIS extension.
				</para>
				<para>
					Now create a table in which the CSV file will be loaded:
					<programlisting>
CREATE TABLE AISInput(
	T	timestamp,
	TypeOfMobile varchar(50),
	MMSI integer,
	Latitude float,
	Longitude float,
	navigationalStatus varchar(50),
	ROT float,
	SOG float,
	COG float,
	Heading integer,
	IMO varchar(50),
	Callsign varchar(50),
	Name varchar(100),
	ShipType varchar(50),
	CargoType varchar(100),
	Width float,
	Length float,
	TypeOfPositionFixingDevice varchar(50),
	Draught float,
	Destination varchar(50),
	ETA varchar(50),
	DataSourceType varchar(50),
	SizeA float,
	SizeB float,
	SizeC float,
	SizeD float,
	Geom geometry(Point, 4326)
);
					</programlisting>
				</para>
			</section>

			<section>
				<title>Loading the Data</title>
				<para>
					For importing CSV data into a PostgreSQL database one can use the <varname>COPY</varname> command as follows:
					<programlisting>
COPY AISInput(T, TypeOfMobile, MMSI, Latitude, Longitude, NavigationalStatus,
ROT, SOG, COG, Heading, IMO, CallSign, Name, ShipType, CargoType, Width, Length,
TypeOfPositionFixingDevice, Draught, Destination, ETA, DataSourceType,
SizeA, SizeB, SizeC, SizeD)
FROM '/home/mobilitydb/DanishAIS/aisdk_20180401.csv' DELIMITER  ',' CSV HEADER;
					</programlisting>
				</para>
				<para>
					This import took about 3 minutes on my machine, which is an average laptop. The CSV file has 10,619,212 rows, all of which were correctly imported. For bigger datasets, one could alternative could use the program <ulink url="https://github.com/dimitri/pgloader">pgloader</ulink>.
				</para>
				<para>
					We clean up some of the fields in the table and create spatial points with the following command.
					<programlisting>
UPDATE AISInput SET
NavigationalStatus = CASE NavigationalStatus WHEN 'Unknown value' THEN NULL END,
IMO = CASE IMO WHEN 'Unknown' THEN NULL END,
ShipType = CASE ShipType WHEN 'Undefined' THEN NULL END,
TypeOfPositionFixingDevice = CASE TypeOfPositionFixingDevice
	WHEN 'Undefined' THEN NULL END,
Geom = ST_SetSRID( ST_MakePoint( Longitude, Latitude ), 4326);
					</programlisting>
				</para>
				<para>
					This took about 5 minutes on my machine. Let's visualize the spatial points on QGIS.
				</para>
				<figure id="imgpoints" float="start"><title>Visualizing the input points</title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/points.png'/></imageobject>
					</mediaobject>
				</figure>
				<para>
					Clearly, there are noise points that are far away from Denmark or even outside earth. This module will not discuss a thorough data cleaning. However, we do some basic cleaning in order to be able to construct trajectories:
					<itemizedlist>
						<listitem>
							<para>Filter out points that are outside the window defined by bounds point(-16.1,40.18) and point(32.88, 84.17). This window is obtained from the specifications of the projection in <ulink url="https://epsg.io/25832">https://epsg.io/25832</ulink>.</para>
						</listitem>
						<listitem>
							<para>Filter out the rows that have the same identifier (MMSI, T)</para>
						</listitem>
					</itemizedlist>
				</para>
				<para>
					<programlisting>
CREATE TABLE AISInputFiltered AS
SELECT DISTINCT ON(MMSI,T) *
FROM AISInput
WHERE Longitude BETWEEN -16.1 and 32.88 AND Latitude BETWEEN 40.18 AND 84.17;
-- Query returned successfully: 10357703 rows affected, 01:14 minutes execution time.
SELECT COUNT(*) FROM AISInputFiltered;
--10357703
					</programlisting>
				</para>
			</section>

			<section>
				<title>Constructing Trajectories</title>
				<para>
					Now we are ready to construct ship trajectories out of their individual observations:
				</para>
				<para>
					<programlisting>
CREATE TABLE Ships(MMSI, Trip, SOG, COG) AS
SELECT MMSI,
	tgeompointseq(array_agg(tgeompointinst( ST_Transform(Geom, 25832), T) ORDER BY T)),
	tfloatseq(array_agg(tfloatinst(SOG, T) ORDER BY T) FILTER (WHERE SOG IS NOT NULL)),
	tfloatseq(array_agg(tfloatinst(COG, T) ORDER BY T) FILTER (WHERE COG IS NOT NULL))
FROM AISInputFiltered
GROUP BY MMSI;
-- Query returned successfully: 2995 rows affected, 01:16 minutes execution time.
					</programlisting>
				</para>
				<para>
					This query constructs, per ship, its spatiotemporal trajectory <varname>Trip</varname>, and two temporal attributes <varname>SOG</varname> and <varname>COG</varname>. <varname>Trip</varname> is a temporal geometry point, and both <varname>SOG</varname> and <varname>COG</varname> are temporal floats. MobilityDB builds on the coordinate transformation feature of PostGIS. Here the SRID 25832 (European Terrestrial Reference System 1989) is used, because it is the one advised by Danish Maritime Authority in the download page of this dataset. Now, let's visualize the constructed trajectories in QGIS.
				</para>
				<para>
					<programlisting>
ALTER TABLE Ships ADD COLUMN Traj geometry;
UPDATE Ships SET Traj= trajectory(Trip);
-- Query returned successfully: 2995 rows affected, 3.8 secs execution time.
					</programlisting>
				</para>
				<para>
					<figure id="imgtrajs" float="start"><title>Visualizing the ship trajectories</title>
						<mediaobject>
							<imageobject><imagedata fileref='workshopimages/trajs.png' /></imageobject>
						</mediaobject>
					</figure>
				</para>
			</section>

			<section>
				<title>Basic Data Exploration</title>
				<para>
					The total distance traveled by all ships:
				</para>
				<para>
					<programlisting>
SELECT SUM( length( Trip ) ) FROM Ships;
--500433519.121321
					</programlisting>
				</para>
				<para>
					This query uses the <varname>length</varname> function to compute per trip the sailing distance in meters. We then aggregate over all trips and calculate the sum. Let's have a more detailed look, and generate a histogram of trip lengths:
				</para>
				<para>
					<programlisting>
WITH buckets (bucketNo, RangeKM) AS (
	SELECT 1, floatrange '[0, 0]' UNION
	SELECT 2, floatrange '(0, 50)' UNION
	SELECT 3, floatrange '[50, 100)' UNION
	SELECT 4, floatrange '[100, 200)' UNION
	SELECT 5, floatrange '[200, 500)' UNION
	SELECT 6, floatrange '[500, 1500)' UNION
	SELECT 7, floatrange '[1500, 10000)' ),
histogram AS (
	SELECT bucketNo, RangeKM, count(MMSI) as freq
	FROM buckets left outer join Ships on (length(Trip)/1000) &#x003C;&commat; RangeKM
	GROUP BY bucketNo, RangeKM
	ORDER BY bucketNo, RangeKM
)
SELECT bucketNo, RangeKM, freq,
	repeat('&#x25AA;', ( freq::float / max(freq) OVER () * 30 )::int ) AS bar
FROM histogram;
--Total query runtime: 5.6 secs

bucketNo,   bucketRange,        freq	   bar
1;          "[0,0]";            303;       &#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;
2;          "(0,50)";           1693;      &#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;
3;          "[50,100)";         267;       &#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;
4;          "[100,200)";        276;       &#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;
5;          "[200,500)";        361;       &#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;&#x25AA;
6;          "[500,1500)";       86;        &#x25AA;&#x25AA;
7;          "[1500,10000)";     6;
					</programlisting>
				</para>
				<para>
					Surprisingly there are trips with zero length. These are clearly noise that can be deleted. Also there are very many short trips, that are less than 50 km long. On the other hand, there are few long trips that are more than 1,500 km long. Let's visualize these last two cases in <xref linkend="imgtrajsshort"/>. They look like noise. Normally one should validate more, but to simplify this module, we consider them as noise, and delete them.
					<programlisting>
DELETE FROM Ships
WHERE length(Trip) = 0 OR length(Trip) >= 1500000;
-- Query returned successfully in 7 secs 304 msec.
					</programlisting>
					Now the <varname>Ships</varname> table looks like <xref linkend="imgtrajsfiltered"/>.
				</para>
				<figure id="imgtrajsshort" float="start"><title>Visualizing trips with abnormal lengths</title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/trajsShort.png'/></imageobject>
					</mediaobject>
				</figure>
				<figure id="imgtrajsfiltered" float="start"><title>Ship trajectories after filtering</title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/trajsFiltered.png'/></imageobject>
					</mediaobject>
				</figure>
				<para>
					Let's have a look at the speed of the ships. There are two speed values in the data; the speed calculated from the spatiotemporal trajectory <varname>speed(Trip)</varname>, and the <varname>SOG</varname> attribute. Optimally, the two will be the same. A small variance would still be OK, because of sensor errors. Note that both are temporal floats. In the next query, we compare the averages of the two speed values for every ship:
					<programlisting>
SELECT ABS(twavg(SOG) * 1.852 - twavg(speed(Trip))* 3.6 ) SpeedDifference
FROM Ships
ORDER BY SpeedDifference DESC;
--Total query runtime: 8.2 secs
--990 rows retrieved.

SpeedDifference
NULL
NULL
NULL
NULL
NULL
107.861100067879
57.1590253627668
42.4207839833568
39.5819188407125
33.6182789410313
30.9078594633161
26.514042447366
22.1312646226031
20.5389022294181
19.8500569368283
19.4134688682774
18.180139457754
17.4859077178001
17.3155991287105
17.1739822139821
12.9571603234404
12.6195380496344
12.2714437568609
10.9619033557275
10.4164745930929
10.3306155308426
9.46457823214455
...
					</programlisting>
				</para>
				<para>
					The <varname>twavg</varname> computes a time-weighted average of a temporal float. It basically computes the area under the curve, then divides it by the time duration of the temporal float. By doing so, the speed values that remain for longer durations affect the average more than those that remain for shorter durations. Note that SOG is in knot, and Speed(Trip) is in m/s. The query converts both to km/h.
				</para>
				<para>
					The query shows that 26 out of the 990 ship trajectories in the table have a difference of more than 10 km/h or NULL. These trajectories are shown in <xref linkend="imgtrajsWrongSpeed"/>. Again they look like noise, so we remove them.
				</para>
				<figure id="imgtrajsWrongSpeed" float="start"><title>Ship trajectories with big difference between <varname>speed(Trip)</varname> and <varname>SOG</varname></title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/trajsWrongSpeed.png' /></imageobject>
					</mediaobject>
				</figure>
				<para>
					Now we do a similar comparison between the calculated azimuth from the spatiotemporal trajectory, and the attribute COG:
					<programlisting>
SELECT ABS(twavg(COG) - twavg(azimuth(Trip)) * 180.0/pi() ) AzimuthDifference
FROM Ships
ORDER BY AzimuthDifference DESC;
--Total query runtime: 4.0 secs
--964 rows retrieved.

264.838740787458
220.958372832234
180.867071483688
178.774337481463
154.239639388087
139.633953692907
137.347542674865
128.239459879571
121.107566199195
119.843262642657
116.685117326047
116.010477588934
109.830338231363
106.94301191915
106.890186229337
106.55297972109
103.20192549283
102.585009756697
...
					</programlisting>
				</para>
				<para>
					Here we see that the COG is not as accurate as the SOG attribute. More than 100 trajectories have an azimuth difference bigger than 45 degrees. <xref linkend="imgtrajsWrongAzimuth"/> visualizes them. Some of them look like noise, but some look fine. For simplicity, we keep them all.
				</para>
				<figure id="imgtrajsWrongAzimuth" float="start"><title>Ship trajectories with big difference between <varname>azimuth(Trip)</varname> and <varname>COG</varname></title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/trajsWrongAzimuth.png'/></imageobject>
					</mediaobject>
				</figure>
			</section>

			<section>
				<title>Analyzing the Trajectories</title>
				<para>
					Now we dive into MobilityDB and explore more of its functions. In <xref linkend="imgtrajFerry"/>, we notice trajectories that keep going between R&oslash;dby and Puttgarden. Most probably, these are the ferries between the two ports. The task is simply to spot which Ships do so, and to count how many one way trips they did in this day. This is expressed in the following query:
				</para>
				<para>
					<programlisting>
CREATE INDEX Ships_Trip_Idx ON Ships USING GiST(Trip);

WITH Ports(Rodby, Puttgarden) AS (
	SELECT ST_MakeEnvelope(651135, 6058230, 651422, 6058548, 25832),
		ST_MakeEnvelope(644339, 6042108, 644896, 6042487, 25832)
)
SELECT S.*, Rodby, Puttgarden
FROM Ports P, Ships S
WHERE intersects(S.Trip, P.Rodby) AND intersects(S.Trip, P.Puttgarden)
--Total query runtime: 462 msec
--4 rows retrieved.
					</programlisting>
				</para>
				<figure id="imgtrajFerry" float="start"><title>A sample ship trajectory between R&oslash;dby and Puttgarden</title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/trajFerry.png'/></imageobject>
					</mediaobject>
				</figure>
				<figure id="imgtrajFerries" float="start"><title>All ferries between R&oslash;dby and Puttgarden</title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/trajFerries.png'/></imageobject>
					</mediaobject>
				</figure>
				<para>
					This query creates two envelope geometries that represent the locations of the two ports, then intersects them with the spatiotemporal trajectories of the ships. The <varname>intersects</varname> function checks whether a temporal point has ever intersects a geometry. To speed up the query, a spatiotemporal GiST index is first built on the <varname>Trip</varname> attribute. The query identified four Ships that commuted between the two ports, <xref linkend="imgtrajFerries"/>. To count how many one way trips each of them did, we extend the previous query as follows:
					<programlisting>
WITH Ports(Rodby, Puttgarden) AS (
	SELECT ST_MakeEnvelope(651135, 6058230, 651422, 6058548, 25832),
		ST_MakeEnvelope(644339, 6042108, 644896, 6042487, 25832)
)
SELECT MMSI, (numSequences(atGeometry(S.Trip, P.Rodby)) +
	numSequences(atGeometry(S.Trip, P.Puttgarden)))/2.0 AS NumTrips
FROM Ports P, Ships S
WHERE intersects(S.Trip, P.Rodby) AND intersects(S.Trip, P.Puttgarden)
--Total query runtime: 1.1 secs

MMSI		NumTrips
219000429;  24.0
211188000;  24.0
211190000;  25.0
219000431;  16.0
					</programlisting>
				</para>
				<para>
					The function <varname>atGeometry</varname> restricts the temporal point to the parts where it is inside the given geometry. The result is thus a temporal point that consists of multiple pieces (sequences), with temporal gaps in between. The function <varname>numSequences</varname> counts the number of these pieces.
				</para>
				<para>
					With this high number of ferry trips, one wonders whether there are collision risks with ships that traverse this belt (the green trips in <xref linkend="imgtrajFerry"/>). To check this, we query whether a pair of ship come very close to one another as follows:
				</para>
				<para>
					<programlisting>
WITH B(Belt) AS (
	SELECT ST_MakeEnvelope(640730, 6058230, 654100, 6042487, 25832)
),
BeltShips AS (
	SELECT MMSI, atGeometry(S.Trip, B.Belt) AS Trip,
		trajectory(atGeometry(S.Trip, B.Belt)) AS Traj
	FROM Ships S, B
	WHERE intersects(S.Trip, B.Belt)
)
SELECT S1.MMSI, S2.MMSI, S1.Traj, S2.Traj, shortestLine(S1.tripETRS, S2.tripETRS) Approach
FROM BeltShips S1, BeltShips S2
WHERE S1.MMSI > S2.MMSI AND
	dwithin(S1.tripETRS, S2.tripETRS, 300)
--Total query runtime: 28.5 secs
--7 rows retrieved.
					</programlisting>
				</para>
				<para>
					The query first defines the area of interest as an envelope, the red dashed line in <xref linkend="imgtrajApproach"/>). It then restricts/crops the trajectories to only this envelope using the <varname>atGeometry</varname> function. The main query then find pairs of different trajectories that ever came within a distance of 300 meters to one another (the <varname>dwithin</varname>). For these trajectories, it computes the spatial line that connects the two instants where the two trajectories were closest to one another (the <varname>shortestLine</varname> function). <xref linkend="imgtrajApproach"/> shows the green trajectories that came close to the blue trajectories, and their shortest connecting line in solid red. Most of the approaches occur at the entrance of the R&oslash;dby port, which looks normal. But we also see two interesting approaches, that may indicate danger of collision away from the port. They are shown with more zoom in <xref linkend="imgApproach1"/> and <xref linkend="imgApproach2"/>
				</para>
				<figure id="imgtrajApproach" float="start"><title>Ship that come closer than 300 meters to one another</title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/trajApproach.png'/></imageobject>
					</mediaobject>
				</figure>
				<figure id="imgApproach1" float="start"><title>A zoom-in on a dangerous approach</title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/approach1.png'/></imageobject>
					</mediaobject>
				</figure>
				<figure id="imgApproach2" float="start"><title>Another dangerous approach</title>
					<mediaobject>
						<imageobject><imagedata width='80%' fileref='workshopimages/approach2.png'/></imageobject>
					</mediaobject>
				</figure>
			</section>
	</chapter>

	<chapter id ="GTFS">
		<title>Managing GTFS Data</title>

		<para>The General Transit Feed Specification (GTFS) defines a common format for public transportation schedules and associated geographic information. GTFS-realtime is used to specify real-time transit data. Many transportation agencies around the world publish their data in GTFS and GTFS-realtime format and make them publicly available. A well-known repository containing such data is  <ulink url="https://transitfeeds.com">OpenMobilityData</ulink>.</para>

		<para>In this chapter, we illustrate how to load GTFS data in MobilityDB. For this, we first need to import the GTFS data into PostgreSQL and then transform this data so that it can be loaded into MobilityDB. The data used in this tutorial is obtained from <ulink url="https://www.stib-mivb.be">STIB-MIVB</ulink>, the Brussels public transportation company and is available as a <ulink url="https://docs.mobilitydb.com/data/gtfs_data.zip">ZIP</ulink> file. You must be aware that GTFS data is typically of big size. In order to reduce the size of the dataset, this file only contains schedules for one week and five transportation lines, whereas typical GTFS data published by STIB-MIVB contains schedules for one month and 99 transportation lines. In the reduced dataset used in this tutorial the final table containing the GTFS data in MobilityDB format has almost 10,000 trips and its size is 241 MB. Furtheremore, we need several temporary tables to transform GTFS format into MobilityDB and these tables are also big, the largest one has almost 6 million rows and its size is 621 MB.</para>

		<para>Several tools can be used to import GTFS data into PostgreSQL. For example, one publicly available in Github can be found <ulink url="https://github.com/fitnr/gtfs-sql-importer">here</ulink>. These tools load GTFS data into PostgreSQL tables, allowing one to perform multiple imports of data provided by the same agency covering different time frames, perform various complex tasks including data validation, and take into account variations of the format provided by different agencies, updates of route information among multiple imports, etc. For the purpose of this tutorial we do a simple import and transformation using only SQL. This is enough for loading the data set we are using but a much more robust solution should be used in an operational environment, if only for coping with the considerable size of typical GTFS data, which would require parallelization of this task.</para>

		<sect1>
			<title>Loading GTFS Data in PostgreSQL</title>

			<para>The <ulink url="https://docs.mobilitydb.com/data/gtfs_data.zip">ZIP</ulink> file with the data for this tutorial contains a set of CSV files (with extension <varname>.txt</varname>) as follows:
				<itemizedlist>
					<listitem>
						<para><varname>agency.txt</varname> contains the description of the transportation agencies provinding the services (a single one in our case).</para>
					</listitem>

					<listitem>
						<para><varname>calendar.txt</varname> contains service patterns that operate recurrently such as, for example, every weekday.</para>
					</listitem>

					<listitem>
						<para><varname>calendar_dates.txt</varname> define exceptions to the default service patterns defined in <varname>calendar.txt</varname>. There are two types of exceptions: 1 means that the service has been added for the specified date, and 2 means that the service has been removed for the specified date.</para>
					</listitem>

					<listitem>
						<para><varname>route_types.txt</varname> contains transportation types used on routes, such as bus, metro, tramway, etc.</para>
					</listitem>

					<listitem>
						<para><varname>routes.txt</varname> contains transit routes. A route is a group of trips that are displayed to riders as a single service.</para>
					</listitem>

					<listitem>
						<para><varname>shapes.txt</varname> contains the vehicle travel paths, which are used to generate the corresponding geometry.</para>
					</listitem>

					<listitem>
						<para><varname>stop_times.txt</varname> contains times at which a vehicle arrives at and departs from stops for each trip.</para>
					</listitem>

					<listitem>
						<para><varname>translations.txt</varname> contains the translation of the route information in French and Dutch. This file is not used in this tutorial.</para>
					</listitem>

					<listitem>
						<para><varname>trips.txt</varname> contains trips for each route. A trip is a sequence of two or more stops that occur during a specific time period.</para>
					</listitem>

				</itemizedlist>
			</para>

			<para>
				We decompress the file with the data into a directory. This can be done using the command.
				<programlisting>
unzip gtfs_data.zip
				</programlisting>
				We suppose in the following that the directory used is as follows <varname>/home/gtfs_tutorial/</varname>.
			</para>

			<para>We create the tables to be loaded with the data in the CSV files as follows.
					<programlisting>
CREATE TABLE agency (
  agency_id text DEFAULT '',
  agency_name text DEFAULT NULL,
  agency_url text DEFAULT NULL,
  agency_timezone text DEFAULT NULL,
  agency_lang text DEFAULT NULL,
  agency_phone text DEFAULT NULL,
  CONSTRAINT agency_pkey PRIMARY KEY (agency_id)
);

CREATE TABLE calendar (
  service_id text,
  monday int NOT NULL,
  tuesday int NOT NULL,
  wednesday int NOT NULL,
  thursday int NOT NULL,
  friday int NOT NULL,
  saturday int NOT NULL,
  sunday int NOT NULL,
  start_date date NOT NULL,
  end_date date NOT NULL,
  CONSTRAINT calendar_pkey PRIMARY KEY (service_id)
);
CREATE INDEX calendar_service_id ON calendar (service_id);

CREATE TABLE exception_types (
  exception_type int PRIMARY KEY,
  description text
);

CREATE TABLE calendar_dates (
  service_id text,
  date date NOT NULL,
  exception_type int REFERENCES exception_types(exception_type)
);
CREATE INDEX calendar_dates_dateidx ON calendar_dates (date);

CREATE TABLE route_types (
  route_type int PRIMARY KEY,
  description text
);

CREATE TABLE routes (
  route_id text,
  route_short_name text DEFAULT '',
  route_long_name text DEFAULT '',
  route_desc text DEFAULT '',
  route_type int REFERENCES route_types(route_type),
  route_url text,
  route_color text,
  route_text_color text,
  CONSTRAINT routes_pkey PRIMARY KEY (route_id)
);

CREATE TABLE shapes (
  shape_id text NOT NULL,
  shape_pt_lat double precision NOT NULL,
  shape_pt_lon double precision NOT NULL,
  shape_pt_sequence int NOT NULL
);
CREATE INDEX shapes_shape_key ON shapes (shape_id);

-- Create a table to store the shape geometries
CREATE TABLE shape_geoms (
  shape_id text NOT NULL,
  shape_geom geometry('LINESTRING', 4326),
  CONSTRAINT shape_geom_pkey PRIMARY KEY (shape_id)
);
CREATE INDEX shape_geoms_key ON shapes (shape_id);

CREATE TABLE location_types (
  location_type int PRIMARY KEY,
  description text
);

CREATE TABLE stops (
  stop_id text,
  stop_code text,
  stop_name text DEFAULT NULL,
  stop_desc text DEFAULT NULL,
  stop_lat double precision,
  stop_lon double precision,
  zone_id text,
  stop_url text,
  location_type integer  REFERENCES location_types(location_type),
  parent_station integer,
  stop_geom geometry('POINT', 4326),
  platform_code text DEFAULT NULL,
  CONSTRAINT stops_pkey PRIMARY KEY (stop_id)
);

CREATE TABLE pickup_dropoff_types (
  type_id int PRIMARY KEY,
  description text
);

CREATE TABLE stop_times (
  trip_id text NOT NULL,
  -- Check that casting to time interval works.
  arrival_time interval CHECK (arrival_time::interval = arrival_time::interval),
  departure_time interval CHECK (departure_time::interval = departure_time::interval),
  stop_id text,
  stop_sequence int NOT NULL,
  pickup_type int REFERENCES pickup_dropoff_types(type_id),
  drop_off_type int REFERENCES pickup_dropoff_types(type_id),
  CONSTRAINT stop_times_pkey PRIMARY KEY (trip_id, stop_sequence)
);
CREATE INDEX stop_times_key ON stop_times (trip_id, stop_id);
CREATE INDEX arr_time_index ON stop_times (arrival_time);
CREATE INDEX dep_time_index ON stop_times (departure_time);

CREATE TABLE trips (
  route_id text NOT NULL,
  service_id text NOT NULL,
  trip_id text NOT NULL,
  trip_headsign text,
  direction_id int,
  block_id text,
  shape_id text,
  CONSTRAINT trips_pkey PRIMARY KEY (trip_id)
);
CREATE INDEX trips_trip_id ON trips (trip_id);

INSERT INTO exception_types (exception_type, description) VALUES
  (1, 'service has been added'),
  (2, 'service has been removed');

INSERT INTO location_types(location_type, description) VALUES
  (0,'stop'),
  (1,'station'),
  (2,'station entrance');

INSERT INTO pickup_dropoff_types (type_id, description) VALUES
  (0,'Regularly Scheduled'),
  (1,'Not available'),
  (2,'Phone arrangement only'),
  (3,'Driver arrangement only');
					</programlisting>
				We created one table for each CSV file. In addition, we created a table <varname>shape_geoms</varname> in order to assemble all segments composing a route into a single geometry and auxiliary tables <varname>exception_types</varname>, <varname>location_types</varname>, and <varname>pickup_dropoff_types</varname> containing acceptable values for some columns in the CSV files.
			</para>

			<para>
				We can load the CSV files into the corresponding tables as follows.
				<programlisting>
COPY calendar(service_id,monday,tuesday,wednesday,thursday,friday,saturday,sunday,
	start_date,end_date) FROM '/home/gtfs_tutorial/calendar.txt' DELIMITER ',' CSV HEADER;
COPY calendar_dates(service_id,date,exception_type)
	FROM '/home/gtfs_tutorial/calendar_dates.txt' DELIMITER ',' CSV HEADER;
COPY stop_times(trip_id,arrival_time,departure_time,stop_id,stop_sequence,
	pickup_type,drop_off_type) FROM '/home/gtfs_tutorial/stop_times.txt' DELIMITER ','
	CSV HEADER;
COPY trips(route_id,service_id,trip_id,trip_headsign,direction_id,block_id,shape_id)
	FROM '/home/gtfs_tutorial/trips.txt' DELIMITER ',' CSV HEADER;
COPY agency(agency_id,agency_name,agency_url,agency_timezone,agency_lang,agency_phone)
	FROM '/home/gtfs_tutorial/agency.txt' DELIMITER ',' CSV HEADER;
COPY route_types(route_type,description)
	FROM '/home/gtfs_tutorial/route_types.txt' DELIMITER ',' CSV HEADER;
COPY routes(route_id,route_short_name,route_long_name,route_desc,route_type,route_url,
	route_color,route_text_color) FROM '/home/gtfs_tutorial/routes.txt' DELIMITER ','
	CSV HEADER;
COPY shapes(shape_id,shape_pt_lat,shape_pt_lon,shape_pt_sequence)
	FROM '/home/gtfs_tutorial/shapes.txt' DELIMITER ',' CSV HEADER;
COPY stops(stop_id,stop_code,stop_name,stop_desc,stop_lat,stop_lon,zone_id,stop_url,
	location_type,parent_station) FROM '/home/gtfs_tutorial/stops.txt' DELIMITER ','
	CSV HEADER;
				</programlisting>
				Finally, we create the geometries for routes and stops as follows.
				<programlisting>
INSERT INTO shape_geoms
SELECT shape_id, ST_MakeLine(array_agg(
	ST_SetSRID(ST_MakePoint(shape_pt_lon, shape_pt_lat),4326) ORDER BY shape_pt_sequence))
FROM shapes
GROUP BY shape_id;

UPDATE stops
SET stop_geom = ST_SetSRID(ST_MakePoint(stop_lon, stop_lat),4326);
				</programlisting>
				The visualization of the routes and stops in QGIS is given in <xref linkend="stib" />. In the figure, red lines correspond to the trajectories of vehicles, while orange points correspond to the location of stops.
			</para>

			<figure id="stib" float="start"><title>Visualization of the routes and stops for the GTFS data from Brussels.</title>
				<mediaobject>
					<imageobject><imagedata scale='35' fileref='workshopimages/stib.png' /></imageobject>
				</mediaobject>
			</figure>
		</sect1>

		<sect1>
			<title>Transforming GTFS Data for MobilityDB</title>
			<para>
				We start by creating a table that contains couples of <varname>service_id</varname> and <varname>date</varname> defining the dates at which a  service is provided.
				<programlisting>
DROP TABLE IF EXISTS service_dates;
CREATE TABLE service_dates AS (
	SELECT service_id, date_trunc('day', d)::date AS date
	FROM calendar c, generate_series(start_date, end_date, '1 day'::interval) AS d
	WHERE (
		(monday = 1 AND extract(isodow FROM d) = 1) OR
		(tuesday = 1 AND extract(isodow FROM d) = 2) OR
		(wednesday = 1 AND extract(isodow FROM d) = 3) OR
		(thursday = 1 AND extract(isodow FROM d) = 4) OR
		(friday = 1 AND extract(isodow FROM d) = 5) OR
		(saturday = 1 AND extract(isodow FROM d) = 6) OR
		(sunday = 1 AND extract(isodow FROM d) = 7)
	)
	EXCEPT
	SELECT service_id, date
	FROM calendar_dates WHERE exception_type = 2
	UNION
	SELECT c.service_id, date
	FROM calendar c JOIN calendar_dates d ON c.service_id = d.service_id
	WHERE exception_type = 1 AND start_date &lt;= date AND date &lt;= end_date
);
				</programlisting>
				This table transforms the service patterns in the <varname>calendar</varname> table valid between a <varname>start_date</varname> and an <varname>end_date</varname> taking into account the week days, and then remove the exceptions of type 2 and add the exceptions of type 1 that are specified in table <varname>calendar_dates</varname>.
			</para>

			<para>
				We now create a table <varname>trip_stops</varname> that determines the stops for each trip.
				<programlisting>
DROP TABLE IF EXISTS trip_stops;
CREATE TABLE trip_stops (
  trip_id text,
  stop_sequence integer,
  no_stops integer,
  route_id text,
  service_id text,
  shape_id text,
  stop_id text,
  arrival_time interval,
  perc float
);

INSERT INTO trip_stops (trip_id, stop_sequence, no_stops, route_id, service_id,
	shape_id, stop_id, arrival_time)
SELECT t.trip_id, stop_sequence, MAX(stop_sequence) OVER (PARTITION BY t.trip_id),
	route_id, service_id, shape_id, stop_id, arrival_time
FROM trips t JOIN stop_times s ON t.trip_id = s.trip_id;

UPDATE trip_stops t
SET perc = CASE
	WHEN stop_sequence =  1 then 0.0
	WHEN stop_sequence =  no_stops then 1.0
	ELSE ST_LineLocatePoint(g.the_geom, s.the_geom)
END
FROM shape_geoms g, stops s
WHERE t.shape_id = g.shape_id AND t.stop_id = s.stop_id;
				</programlisting>
				We perform a join between <varname>trips</varname> and <varname>stop_times</varname> and determines the number of stops in a trip. Then, we compute the relative location of a stop within a trip using the function <varname>ST_LineLocatePoint</varname>.
			</para>

			<para>
				We now create a table <varname>trip_segs</varname> that defines the segments between two consecutive stops of a trip.
				<programlisting>
DROP TABLE IF EXISTS trip_segs;
CREATE TABLE trip_segs (
	trip_id text,
	route_id text,
	service_id text,
	stop1_sequence integer,
	stop2_sequence integer,
	no_stops integer,
	shape_id text,
	stop1_arrival_time interval,
	stop2_arrival_time interval,
	perc1 float,
	perc2 float,
	seg_geom geometry,
	seg_length float,
	no_points integer,
	PRIMARY KEY (trip_id, stop1_sequence)
);

INSERT INTO trip_segs (trip_id, route_id, service_id, stop1_sequence, stop2_sequence,
	no_stops, shape_id, stop1_arrival_time, stop2_arrival_time, perc1, perc2)
WITH temp AS (
	SELECT trip_id, route_id, service_id, stop_sequence,
		LEAD(stop_sequence) OVER w AS stop_sequence2,
		MAX(stop_sequence) OVER (PARTITION BY trip_id),
		shape_id, arrival_time, LEAD(arrival_time) OVER w, perc, LEAD(perc) OVER w
	FROM trip_stops WINDOW w AS (PARTITION BY trip_id ORDER BY stop_sequence)
)
SELECT * FROM temp WHERE stop_sequence2 IS NOT null;

UPDATE trip_segs t
SET seg_geom = ST_LineSubstring(g.the_geom, perc1, perc2)
FROM shape_geoms g
WHERE t.shape_id = g.shape_id;

UPDATE trip_segs
SET seg_length = ST_Length(seg_geom), no_points = ST_NumPoints(seg_geom);
				</programlisting>
				We use twice the <varname>LEAD</varname> window function for obtaning the next stop and the next percentage of a given stop and the <varname>MAX</varname> window function for obtaining the total number of stops in a trip. Then, we generate the geometry of the segment betwen two stops using the function <varname>ST_LineSubstring</varname> and compute the length and the number of points in the segment with functions <varname>ST_Length</varname> and <varname>ST_NumPoints</varname>.
			</para>

			<para>
				The geometry of a segment is a linestring containing multiple points. From the previous table we know at which time the trip arrived at the first point and at the last point of the segment. To determine at which time the trip arrived at the intermediate points of the segments, we create a table <varname>trip_points</varname> that contains all the points composing the geometry of a segment.
				<programlisting>
DROP TABLE IF EXISTS trip_points;
CREATE TABLE trip_points (
	trip_id text,
	route_id text,
	service_id text,
	stop1_sequence integer,
	point_sequence integer,
	point_geom geometry,
	point_arrival_time interval,
	PRIMARY KEY (trip_id, stop1_sequence, point_sequence)
);

INSERT INTO trip_points (trip_id, route_id, service_id, stop1_sequence,
	point_sequence, point_geom, point_arrival_time)
WITH temp1 AS (
	SELECT trip_id, route_id, service_id, stop1_sequence, stop2_sequence,
		no_stops, stop1_arrival_time, stop2_arrival_time, seg_length,
		(dp).path[1] AS point_sequence, no_points, (dp).geom as point_geom
	FROM trip_segs, ST_DumpPoints(seg_geom) AS dp
),
temp2 AS (
	SELECT trip_id, route_id, service_id, stop1_sequence, stop1_arrival_time,
		stop2_arrival_time, seg_length, point_sequence, no_points, point_geom
	FROM temp1
	WHERE point_sequence &lt;&gt; no_points OR stop2_sequence = no_stops
),
temp3 AS (
	SELECT trip_id, route_id, service_id, stop1_sequence, stop1_arrival_time,
		stop2_arrival_time, point_sequence, no_points, point_geom,
		ST_Length(ST_MakeLine(array_agg(point_geom) OVER w)) / seg_length AS perc
	FROM temp2 WINDOW w AS (PARTITION BY trip_id, service_id, stop1_sequence
		ORDER BY point_sequence)
)
SELECT trip_id, route_id, service_id, stop1_sequence, point_sequence, point_geom,
	CASE
	WHEN point_sequence = 1 then stop1_arrival_time
	WHEN point_sequence = no_points then stop2_arrival_time
	ELSE stop1_arrival_time + ((stop2_arrival_time - stop1_arrival_time) * perc)
	END AS point_arrival_time
FROM temp3;
				</programlisting>
				In the temporary table <varname>temp1</varname> we use the function <varname>ST_DumpPoints</varname> to obtain the points composing the geometry of a segment. Nevertheless, this table contains duplicate points, that is, the last point of a segment is equal to the first point of the next one. In the temporary table <varname>temp2</varname> we filter out the last point of a segment unless it is the last segment of the trip. In the temporary table <varname>temp3</varname> we compute in the attribute <varname>perc</varname> the relative position of a point within a trip segment with window functions. For this we use the function <varname>ST_MakeLine</varname> to construct the subsegment from the first point of the segment to the current one, determine the length of the subsegment with function <varname>ST_Length</varname> and divide this length by the overall segment length. Finally, in the outer query we use the computed percentage to determine the arrival time to that point.
			</para>

			<para>
				Our last temporary table <varname>trips_input</varname> contains the data in the format that can be used for creating the MobilityDB trips.
				<programlisting>
DROP TABLE IF EXISTS trips_input;
CREATE TABLE trips_input (
	trip_id text,
	route_id text,
	service_id text,
	date date,
	point_geom geometry,
	t timestamptz
);

INSERT INTO trips_input
SELECT trip_id, route_id, t.service_id, date, point_geom, date + point_arrival_time AS t
FROM trip_points t JOIN
	( SELECT service_id, MIN(date) AS date FROM service_dates GROUP BY service_id) s
	ON t.service_id = s.service_id;
				</programlisting>
				In the inner query of the <varname>INSERT</varname> statement, we select the first date of a service in the <varname>service_dates</varname> table and then we join the resulting table with the <varname>trip_points</varname> table to compute the arrival time at each point composing the trips. Notice that we filter the first date of each trip for optimization purposes because in the next step below we use the <varname>shift</varname> function to compute the trips to all other dates. Alternatively, we could join the two tables but this will be considerably slower for big GTFS files.
			</para>

			<para>
				Finally, table <varname>trips_mdb</varname> contains the MobilityDB trips.
				<programlisting>
DROP TABLE IF EXISTS trips_mdb;
CREATE TABLE trips_mdb (
	trip_id text NOT NULL,
	route_id text NOT NULL,
	date date NOT NULL,
	trip tgeompoint,
	PRIMARY KEY (trip_id, date)
);

INSERT INTO trips_mdb(trip_id, route_id, date, trip)
SELECT trip_id, route_id, date, tgeompointseq(array_agg(tgeompointinst(point_geom, t)
	ORDER BY T))
FROM trips_input
GROUP BY trip_id, route_id, date;

INSERT INTO trips_mdb(trip_id, service_id, route_id, date, trip)
SELECT trip_id, route_id, t.service_id, d.date,
	shift(trip, make_interval(days => d.date - t.date))
FROM trips_mdb t JOIN service_dates d ON t.service_id = d.service_id AND t.date &lt;&gt; d.date;
				</programlisting>
				In the first <varname>INSERT</varname> statement we group the rows in the <varname>trips_input</varname> table by <varname>trip_id</varname> and <varname>date</varname> while keeping the <varname>route_id</varname> atribute, use the <varname>array_agg</varname> function to construct an array containing the temporal points composing the trip ordered by time, and compute the trip from this array using the function <varname>tgeompointseq</varname>. As explained above, table <varname>trips_input</varname> only contains the first date of a trip. In the second <varname>INSERT</varname> statement we add the trips for all the other dates with the function <varname>shift</varname>.
			</para>
		</sect1>

	</chapter>

	<chapter id ="location_history">
		<title>Managing Google Location History</title>

		<sect1 id ="loading_location_history">
			<title>Loading Google Location History Data</title>
			<para>By activating the Location History in your Google account, you let Google track where you go with every mobile device. You can view and manage your Location History information through Google Maps Timeline. The data is provided in JSON format. An example of such a file is as follows.
				<programlisting>
{
  "locations" : [ {
    "timestampMs" : "1525373187756",
    "latitudeE7" : 508402936,
    "longitudeE7" : 43413790,
    "accuracy" : 26,
    "activity" : [ {
      "timestampMs" : "1525373185830",
      "activity" : [ {
        "type" : "STILL",
        "confidence" : 44
      }, {
        "type" : "IN_VEHICLE",
        "confidence" : 16
      }, {
        "type" : "IN_ROAD_VEHICLE",
        "confidence" : 16
      }, {
        "type" : "UNKNOWN",
        "confidence" : 12
      }, {
        "type" : "IN_RAIL_VEHICLE",
        "confidence" : 12
...
				</programlisting>
			</para>

			<para>If we want to load location information into MobilityDB we only need the fields <varname>longitudeE7</varname>, <varname>latitudeE7</varname>, and <varname>timestampMs</varname>. To convert the original JSON file into a CSV file containing only these fields we can use <ulink url="https://stedolan.github.io/jq/">jq</ulink>, a command-line JSON processor. The following command
				<programlisting>
cat location_history.json | jq -r ".locations[] | {latitudeE7, longitudeE7, timestampMs}
| [.latitudeE7, .longitudeE7, .timestampMs] | @csv" > location_history.csv
				</programlisting>
				produces a CSV file of the following format
				<programlisting>
508402936,43413790,"1525373187756"
508402171,43413455,"1525373176729"
508399229,43413304,"1525373143463"
508377525,43411499,"1525373113741"
508374906,43412597,"1525373082542"
508370337,43418136,"1525373052593"
...
				</programlisting>
				The above command works well for files of moderate size since by default jq loads the whole input text in memory. For very large files you may consider the <varname>--stream</varname> option of jq, which parses input texts in a streaming fashion.
			</para>

			<para>Now we can import the generated CSV file into PostgreSQL as follows.
				<programlisting>
DROP TABLE IF EXISTS location_history;
CREATE TABLE location_history (
	latitudeE7 float,
	longitudeE7 float,
	timestampMs bigint,
	date date
);

COPY location_history(latitudeE7, longitudeE7, timestampMs) FROM
	'/home/location_history/location_history.csv' DELIMITER ',' CSV;

UPDATE location_history
SET date = date(to_timestamp(timestampMs / 1000.0)::timestamptz);
				</programlisting>
				Notice that we added an attribute <varname>date</varname> to the table so we can split the full location history, which can comprise data for several years, by date. Since the timestamps are encoded in milliseconds since 1/1/1970, we divide them by 1,000 and apply the functions <varname>to_timestamp</varname> and <varname>date</varname> to obtain corresponding date.
			</para>

			<para>
				We can now transform this data into MobilityDB trips as follows.
				<programlisting>
DROP TABLE IF EXISTS locations_mdb;
CREATE TABLE locations_mdb (
	date date NOT NULL,
	trip tgeompoint,
	trajectory geometry,
	PRIMARY KEY (date)
);

INSERT INTO locations_mdb(date, trip)
SELECT date, tgeompointseq(array_agg(tgeompointinst(
	ST_SetSRID(ST_Point(longitudeE7/1e7, latitudeE7/1e7),4326),
	to_timestamp(timestampMs / 1000.0)::timestamptz) ORDER BY timestampMs))
FROM location_history
GROUP BY date;

UPDATE locations_mdb
SET trajectory = trajectory(trip);
				</programlisting>
				We convert the longitude and latitude values into standard coordinates values by dividing them by 10<superscript>7</superscript>. These can be converted into PostGIS points in the WGS84 coordinate system with the functions <varname>ST_Point</varname> and <varname>ST_SetSRID</varname>. Also, we convert the timestamp values in miliseconds to <varname>timestamptz</varname> values. We can now apply the function
				<varname>tgeompointinst</varname> to create a <varname>tgeompoint</varname> of instant duration from the point and the timestamp, collect all temporal points of a day into an array with the function <varname>array_agg</varname>, and finally, create a temporal point containing all the locations of a day using function <varname>tgeompointseq</varname>. We added to the table a <varname>trajectory</varname> attribute to visualize the location history in QGIS is given in <xref linkend="location_history_fig" />.
			</para>


			<figure id="location_history_fig" float="start"><title>Visualization in QGIS of the Google location history loaded into MobilityDB.</title>
				<mediaobject>
					<imageobject><imagedata fileref='workshopimages/location_history.png' /></imageobject>
				</mediaobject>
			</figure>

		</sect1>

	</chapter>

	<chapter id ="GPX">
		<title>Managing GPX Data</title>

		<sect1 id ="loading_gpx">
			<title>Loading GPX Data</title>

			<para>
				GPX, or GPS Exchange Format, is an XML data format for GPS data. Location data (and optionally elevation, time, and other information) is stored in tags and can be interchanged between GPS devices and software. Conceptually, a GPX file contains tracks, which are a record of where a moving object has been, and routes, which are suggestions about where it might go in the future. Furthermore, both tracks and routes and composed by points. The following is a truncated (for brevity) example GPX file.
				<programlisting>
&lt;?xml version='1.0' encoding='UTF-8' standalone='yes' ?&gt;
&lt;gpx version="1.1"
	xmlns="http://www.topografix.com/GPX/1/1"
	xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xsi:schemaLocation="http://www.topografix.com/GPX/1/1
	http://www.topografix.com/GPX/1/1/gpx.xsd"
	creator="Example creator"&gt;
 	&lt;metadata&gt;
		&lt;name&gt;Dec 14, 2014 4:32:04 PM&lt;/name&gt;
		&lt;author&gt;Example creator&lt;/author&gt;
		&lt;link href="https://..." /&gt;
		&lt;time&gt;2014-12-14T14:32:04.650Z&lt;/time&gt;
	&lt;/metadata&gt;
	&lt;trk&gt;
		&lt;name&gt;Dec 14, 2014 4:32:04 PM&lt;/name&gt;
		&lt;trkseg&gt;
			&lt;trkpt lat="30.16398" lon="31.467701"&gt;
				&lt;ele&gt;76&lt;/ele&gt;
				&lt;time&gt;2014-12-14T14:32:10.339Z&lt;/time&gt;
			&lt;/trkpt&gt;
			&lt;trkpt lat="30.16394" lon="31.467333"&gt;
				&lt;ele&gt;73&lt;/ele&gt;
				&lt;time&gt;2014-12-14T14:32:16.00Z&lt;/time&gt;
			&lt;/trkpt&gt;
			&lt;trkpt lat="30.16408" lon="31.467218"&gt;
				&lt;ele&gt;74&lt;/ele&gt;
				&lt;time&gt;2014-12-14T14:32:19.00Z&lt;/time&gt;
			&lt;/trkpt&gt;
			[...]
		&lt;/trkseg&gt;
		&lt;trkseg&gt;
			[...]
		&lt;/trkseg&gt;
		[...]
	&lt;/trk&gt;
	&lt;trk&gt;
		[...]
	&lt;/trk&gt;
	[...]
&lt;gpx&gt;
				</programlisting>
			</para>

			<para>
				The following Python program called <varname>gpx_to_csv.py</varname> uses <varname>expat</varname>, a stream-oriented XML parser library, to convert the above GPX file in CSV format.
				<programlisting>
import sys
import xml.parsers.expat

stack = []
def start_element(name, attrs):
	stack.append(name)
	if name == 'gpx' :
		print("lon,lat,time")
	if name == 'trkpt' :
		print("{},{},".format(attrs['lon'], attrs['lat']), end="")

def end_element(name):
	stack.pop()

def char_data(data):
	if stack[-1] == "time" and stack[-2] == "trkpt" :
		print(data)

p = xml.parsers.expat.ParserCreate()

p.StartElementHandler = start_element
p.EndElementHandler = end_element
p.CharacterDataHandler = char_data

p.ParseFile(sys.stdin.buffer)
				</programlisting>
			</para>

			<para>
				This Python program can be executed as follows.
				<programlisting>
python3 gpx_to_csv.py &lt; example.gpx &gt; example.csv
				</programlisting>
				The resulting CSV file is given next.
				<programlisting>
lon,lat,time
31.46032,30.037502,2015-02-09T08:10:16.00Z
31.460901,30.039026,2015-02-09T08:10:31.00Z
31.461981,30.039816,2015-02-09T08:10:57.00Z
31.461996,30.039801,2015-02-09T08:10:58.00Z
...
				</programlisting>
				The above CSV file can be loaded into MobilityDB as follows.
				<programlisting>
DROP TABLE IF EXISTS trips_input;
CREATE TABLE trips_input (
	date date,
	lon float,
	lat float,
	time timestamptz
);

COPY trips_input(lon, lat, time) FROM
	'/home/gpx_data/example.csv' DELIMITER ',' CSV HEADER;

UPDATE trips_input
SET date = date(time);

DROP TABLE IF EXISTS trips_mdb;
CREATE TABLE trips_mdb (
	date date NOT NULL,
	trip tgeompoint,
	trajectory geometry,
	PRIMARY KEY (date)
);

INSERT INTO trips_mdb(date, trip)
SELECT date, tgeompointseq(array_agg(tgeompointinst(
	ST_SetSRID(ST_Point(lon, lat),4326), time) ORDER BY time))
FROM trips_input
GROUP BY date;

UPDATE trips_mdb
SET trajectory = trajectory(trip);
				</programlisting>
			</para>

		</sect1>
	</chapter>
</book>
